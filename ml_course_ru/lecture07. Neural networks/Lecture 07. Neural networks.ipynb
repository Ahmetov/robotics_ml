{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лекция 07. Нейронная сеть\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Нелинейная задача классификации\n",
    "Когда мы хотим использовать машинное обучение для построения классификатора изображений автомобилей, нам нужен обучающий набор данных с истинными метками, автомобиль или нет. После процесса обучения мы получаем хороший классификатор. Когда мы тестируем его с новым изображением, классификатор ответит, является ли это новое изображение автомобилем или нет.\n",
    "![](../../img/lec07_01.png)\n",
    "\n",
    "Прежде чем двигаться дальше, нам нужно знать, как компьютер \"видит\" картинку. Как и изображение справа, компьютер всегда \"видит\" изображение в виде группы пикселей со значениями интенсивности. Например, на рисунке показано положение пикселя (красная точка), а его значение интенсивности равно 69.\n",
    "\n",
    "![](../../img/lec07_02.png)\n",
    "\n",
    "Теперь давайте посмотрим на процесс обучения. Во-первых, возьмите два пикселя в качестве объектов.\n",
    "\n",
    "![](../../img/lec07_03.png)\n",
    "\n",
    "Во-вторых, определите **нелинейную логистическую регрессию** как гипотезу ***H***. Наша цель состоит в том, чтобы найти хороший ***H***, который может хорошо различать положительные и отрицательные данные.\n",
    "\n",
    "![](../../img/lec07_04.png)\n",
    "\n",
    "Наконец, изучите все параметры ***H***. Сравните их с линейной логистической регрессией; нелинейная форма более сложна, так как она имеет много полиномиальных членов.\n",
    "\n",
    "Однако, когда число признаков велико, приведенное выше решение не является хорошим выбором для изучения сложной нелинейной гипотезы. Предположим, что у нас есть изображение размером 50х50 пикселей и все пиксели являются объектами, следовательно, нелинейная гипотеза должна иметь более 2500 объектов, поскольку ***H*** имеет дополнительные квадратичные или кубические объекты. Вычислительная стоимость была бы очень дорогой, чтобы найти все параметры $\\theta$ (или $W$, как ранее) этих функций в обучающих данных.\n",
    "\n",
    "Поэтому нам нужен лучший способ - нейронная сеть, которая является очень мощной и широко используемой моделью для изучения сложных нелинейных гипотез во многих примерах."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Нейронная сеть (Neural Network (NN))\n",
    "\n",
    "В этом разделе мы поговорим о том, как визуализировать гипотезу при использовании нейронных сетей.\n",
    "\n",
    "Первоначально нейронная сеть - это алгоритм, вдохновленный человеческим мозгом, и который пытается имитировать человеческий мозг. Подобно нейронам человеческого мозга, NN имеет множество взаимосвязанных узлов (так называемых нейронов), которые организованы в слои.\n",
    "\n",
    "### Простейшая нейронная сеть\n",
    "\n",
    "Эта простейшая NN-модель содержит только один нейрон. **Мы можем рассматривать нейрон (узел) как логистическую единицу с *сигмоидной (логистической) активационной функцией****,* которая может вычеслить базовое значение, основанное на сигмоидной активационной функции.\n",
    "\n",
    "* Терминология параметров $\\theta$ in ** NN ** называется *** весами***\n",
    "* В зависимости от задачи вы можете решить, следует ли использовать блоки смещения или нет.\n",
    "\n",
    "![](../../img/lec07_05.png)\n",
    "\n",
    "### Нейронная сеть (NN)\n",
    "\n",
    "* Слой 1 называется **Input Layer (входным слоем)**, который вводит объекты.\n",
    "* Последний слой называется **Output Layer (выходным слоем)**, который выводит конечное значение, вычисленное по гипотезе ***H***.\n",
    "* Слой между входным и выходным слоями называется скрытым слоем, который представляет собой блок, в котором мы группируем нейроны вместе.\n",
    "\n",
    "![](../../img/lec07_06.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Заметка\n",
    "\n",
    "Нейрон (узел) на самом деле является логистической единицей с сигмоидной(логистической) активационной функцией (т. е. простой логистической регрессией). Если вы знакомы с линейной алгеброй, вы можете представить себе скрытый слой как линейную комбинацию узлов предыдущего слоя. ***Таким образом, основная идея NN заключается в решении сложной нелинейной задачи классификации с использованием многих последовательностей простой логистической регрессии.**\n",
    "\n",
    "Нейрон (узел) на самом деле является логистической единицей с сигмоидной(логистической) активационной функцией (т. е. простой логистической регрессией). Если вы знакомы с линейной алгеброй, вы можете представить себе скрытый слой как линейную комбинацию узлов предыдущего слоя. ***Таким образом, основная идея NN заключается в решении сложной нелинейной задачи классификации с использованием многих последовательностей простой логистической регрессии.**\n",
    "\n",
    "Чтобы получить четкое представление о том, что делает эта нейронная сеть, давайте пройдем через вычислительные шаги и визуализируем их.\n",
    "\n",
    "**Во-первых**, мы визуализируем процесс перехода матрицы $\\theta$, которая является отображением управляющей функции из слоя *j* в *j+1.*\n",
    "\n",
    "![](../../img/lec07_07.png)\n",
    "![](../../img/lec07_08.png)\n",
    "\n",
    "**Во-вторых**, мы визуализируем каждый процесс вычисления нейронов. Заметим, что выходное значение каждого нейрона вычисляется по его сигмовидной функции активации.\n",
    "\n",
    "![](../../img/lec07_09.png)\n",
    "![](../../img/lec07_10.png)\n",
    "![](../../img/lec07_11.png)\n",
    "![](../../img/lec07_12.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###  Другие архитектуры нейронных сетей\n",
    "\n",
    "Другие архитектуры нейронных сетей могут быть разработаны путем расширения скрытых слоев. Количество нейронов в каждом слое будет зависеть от решаемой задачи.\n",
    "\n",
    "![](../../img/lec07_13.png)\n",
    "\n",
    "### Использование и примеры\n",
    "\n",
    "Основываясь на вышеизложенной концепции, мы собираемся разработать NN таким образом, чтобы показать, как NN-модель может быть применена к нелинейным задачам классификации.\n",
    "\n",
    "#### Пример1 — AND\n",
    "\n",
    "Мы можем спроектировать простую NN с одним нейроном для решения AND задачи.  Учитывая -30, 20 и 20 в качестве весов, можно задать *сигмоидную функцию активации* ***H*** этого нейрона (узла). Когда мы прогнозируем данные с помощью этого ***H***, мы можем получить идеальный результат.\n",
    "\n",
    "![](../../img/lec07_14.png)\n",
    "![](../../img/lec07_15.png)\n",
    "\n",
    "\n",
    "#### Пример2 — OR\n",
    "\n",
    "Концепция работы OR аналогична AND, но мы меняем вес блока смещения на -10.\n",
    "\n",
    "![](../../img/lec07_16.png)\n",
    "![](../../img/lec07_17.png)\n",
    "\n",
    "#### Пример3 — Negation\n",
    "![](../../img/lec07_18.png)\n",
    "\n",
    "#### Пример4 — NAND\n",
    "![](../../img/lec07_19.png)\n",
    "\n",
    "#### Пример5 — XOR\n",
    "\n",
    "Ясно, что это нелинейная задача классификации, мы можем решить ее с помощью нелинейной логистической регрессии ***H***, которую мы обсуждали в начале.\n",
    "\n",
    "![](../../img/lec07_20.png)\n",
    "![](../../img/lec07_21.png)\n",
    "\n",
    "Однако, когда количество объектов и данных велико, ***H*** будет слишком сложным для понимания, а вычислительные затраты - дорогостоящими. \n",
    "\n",
    "**Вместо этого мы используем структуру NN, чтобы сделать модель *H* более ясной и простой.** Здесь мы ****** применим NN к зпдаче XOR на основе AND, NAND и OR.\n",
    "\n",
    "![](../../img/lec07_22.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Проблема многоклассовой классификации\n",
    "\n",
    "В этом случае мы можем добавить узлы в выходной слой, каждый узел может предсказать один класс, концепция здесь будет аналогична механизму one-vs-all.\n",
    "\n",
    "![](../../img/lec07_23.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Прямое распространение\n",
    "\n",
    "Шаги описаны ниже:\n",
    "\n",
    "* Во-первых, нам нужна матрица весов $\\theta$ (или обозначенная W) для каждого скрытого слоя. Мы можем вывести их случайным образом или на основе предварительного знания.\n",
    "* Начните с входного слоя X (или обозначенного $a^1$).\n",
    "* Сделайте прямое распространение по всему выходному слою и получите конечное значение.\n",
    "* Используйте конечное значение для прогнозирования.\n",
    "\n",
    "![](../../img/lec07_24.png)\n",
    "\n",
    "\n",
    "### Больше деталей\n",
    "\n",
    "Давайте возьмем в качестве примера задачу бинарной классификации и визуализируем детали прямого распространения. Прежде всего, нам нужна законченная модель NN, то есть все весовые матрицы **W** уже известны.\n",
    "\n",
    "Входной слой: 4 узла, выходной слой: 1 узел, скрытый слой : 6 узлов.\n",
    "\n",
    "![](../../img/lec07_25.png)\n",
    "![](../../img/lec07_26.png)\n",
    "\n",
    "**Прямое распространение (1) - от входного слоя к выходному слою**\n",
    "\n",
    "![](../../img/lec07_27.png)\n",
    "![](../../img/lec07_28.png)\n",
    "\n",
    "**Прямое распространение(2) - от скрытого слоя к выходному слою**\n",
    "\n",
    "![](../../img/lec07_29.png)\n",
    "\n",
    "В итоге, мы можем получить конечное значение из выходного слоя и использовать его для прогнозирования.\n",
    "\n",
    "* Если конечное значение ≥ 0,5, то мы прогнозируем метку y=1.\n",
    "* Если нет, то мы предсказываем метку y=0.\n",
    "\n",
    "![](../../img/lec07_30.png)\n",
    "\n",
    "### **Проблема мультиклассификации**\n",
    "\n",
    "В задаче мультиклассификации в выходном слое имеется несколько единиц измерения. Каждая единица измерения представляет собой **степень уверенности в принадлежности данных к определенной категории.** В этой ситуации мы можем применить стратегию \"Один против всех\" (one-vs-all), чтобы получить результат прогнозирования. Обратите внимание, что прямое распространение совпадает с проблемой двоичной классификации.\n",
    "\n",
    "![](../../img/lec07_31.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Loss Function (функция потерь)\n",
    "\n",
    "Функция потерь также называется функцией затрат (Cost Function), но это имя чаще используется в NN.\n",
    "\n",
    "Цель функции потерь состоит в том, чтобы измерить ошибку, которую сделала модель. Здесь мы приводим сгенерированную формулу относительно функции потерь. Отметим, что узел NN является логистической единицей с сигмоидной (логистической) активационной функцией.\n",
    "\n",
    "![](../../img/lec07_32.png)\n",
    "\n",
    "## Backward Propagation (обратное распространение)\n",
    "\n",
    "Эта процедура предназначена для **минимизации функции потерь**, как и метод градиентного спуска, который мы использовали до этого.\n",
    "\n",
    "Способ, который мы использовали здесь, похож на градиентный спуск, где есть два шага:\n",
    "\n",
    "1. Вычислите частичную производную от $J(\\theta)$ или $J(W)$\n",
    "2. Обновите каждый элемент матрицы весов **$\\theta$**.\n",
    "\n",
    "![](../../img/lec07_33.png)\n",
    "\n",
    "## Понимание с помощью интуитивного способа (Intuitive Way)\n",
    "\n",
    "Давайте визуализируем процедуру, используя результат частной производной от $J(\\theta)$\n",
    "\n",
    "Давайте начнем с другого примера, предполагая, что матрицы весов **$\\theta$** (или обозначенные **$W$**) были инициализированы. Наша цель - минимизировать **$J(\\theta)$** и затем обновить матрицы весов **$\\theta$**.\n",
    "\n",
    "Примечание - мы можем использовать случайный метод для инициализации весовых матриц Θ, если у нас нет предварительных данных для решения задачи.\n",
    "\n",
    "![](../../img/lec07_34.png)\n",
    "\n",
    "После анализа (Calculus) вычислений мы можем получить результат частной производной от **$J(\\theta)$**\n",
    "\n",
    "![](../../img/lec07_35.png)\n",
    "![](../../img/lec07_36.png)\n",
    "\n",
    "Давайте углубимся в значения **$\\delta$**-   \n",
    "\n",
    "* **$\\delta^3$** - это ошибка 3 слоя, выходного слоя в данном примере.\n",
    "* **$\\delta^2$** - это ошибка 2 слоя, скрытого слоя в данном примере.\n",
    "* **$\\delta^1$** - не существует, так как 1 слой является входным слоем.\n",
    "\n",
    "![](../../img/lec07_37.png)\n",
    "![](../../img/lec07_38.png)\n",
    "\n",
    "С результатом частной производной от $J(\\theta)$ теперь можно обновить матрицу весов Θ.\n",
    "\n",
    "![](../../img/lec07_39.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Подробнее о частной производной $J(\\theta)$\n",
    "\n",
    "В этом разделе мы хотим дать некоторые подсказки о выводе приведенной выше формулировки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![](../../img/lec07_40.png)\n",
    "\n",
    "Для удобства мы используем тот же самый пример и предположим, что есть только один набор данных. Следовательно, функция потерь $J(\\theta)$ проще, чем общая форма.\n",
    "\n",
    "![](../../img/lec07_41.png)\n",
    "\n",
    "Для того, чтобы сделать обратное распространение, нам нужно сначала сделать прямое распространение.\n",
    "\n",
    "![](../../img/lec07_42.png)\n",
    "\n",
    "Тогда мы можем сделать частичную производную от J(Θ). Здесь мы покажем частную производную двух элементов W1 и W2 соответственно. Если вам интересно, вы можете решить ее с помощью подсказок.\n",
    "\n",
    "![](../../img/lec07_43.png)\n",
    "\n",
    "![](../../img/lec07_44.png)\n",
    "\n",
    "Наконец, вы можете доказать формулу частной производной от J(Θ), если выполните описанную выше процедуру. Не беспокойтесь об этом, если вы не знакомы с исчислением, мы дали вам достаточно понятий в разделе ***понимание с интуитивным способом.***\n",
    "\n",
    "## Как реализовать NN на практике?\n",
    "\n",
    "<a href=\"https://medium.com/@qempsil0914/implement-neural-network-without-using-deep-learning-libraries-step-by-step-tutorial-python3-e2aa4e5766d1\" class= \"bb cn kh ki kj kk\" >Вот ссылка на упражнение</a>, мы реализуем NN без использования библиотеки глубокого обучения. Мы надеемся, что учебник может помочь вам понять архитектуру NN, прямую и обратную процедуры распространения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ссылки: https://medium.com/@qempsil0914/courseras-machine-learning-notes-week5-neural-network-lost-function-forward-and-backward-8b293401e4dc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
